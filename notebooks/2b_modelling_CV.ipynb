{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc\\OneDrive\\projects\\Kaggle\\cmi-behavior-sensor-data\\.venv\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, glob\n",
    "import gc\n",
    "import time, math, random\n",
    "import ast\n",
    "from itertools import product, combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, KFold, GroupShuffleSplit, GroupKFold, ParameterGrid\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import validation_curve, ValidationCurveDisplay\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from scipy.stats import rankdata\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "sys.path.insert(0, os.path.abspath('../scripts'))\n",
    "from cmi_2025 import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"..\"\n",
    "COMP_DATA_BASE = os.path.join(BASE, \"data\", \"raw\")\n",
    "PREP_DATA_BASE = os.path.join(BASE, \"data\", \"processed\")\n",
    "FIGURES_BASE = os.path.join(BASE, \"figures\")\n",
    "\n",
    "TRAIN_PATH = os.path.join(COMP_DATA_BASE, \"train.csv\")\n",
    "TRAIN_DEMO_PATH = os.path.join(COMP_DATA_BASE, \"train_demographics.csv\")\n",
    "TEST_PATH = os.path.join(COMP_DATA_BASE, \"test.csv\")\n",
    "TEST_DEMO_PATH = os.path.join(COMP_DATA_BASE, \"test_demographics.csv\")\n",
    "\n",
    "features = list(pl.read_csv(TRAIN_PATH).select(pl.all().exclude(\"ID\")).columns)\n",
    "train_ds = pl.read_csv(TRAIN_PATH)\n",
    "train_demo_ds = pl.read_csv(TRAIN_DEMO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc\\OneDrive\\projects\\Kaggle\\cmi-behavior-sensor-data\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:129: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id-cols: ['row_id', 'sequence_id', 'sequence_counter', 'subject']\n",
      "acc: ['acc_x', 'acc_y', 'acc_z']\n",
      "rot: ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
      "thm: ['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']\n",
      "tof_1: ['tof_1_v0', 'tof_1_v1', 'tof_1_v2', 'tof_1_v3', 'tof_1_v4', 'tof_1_v5', 'tof_1_v6', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63']\n",
      "tof_2: ['tof_2_v0', 'tof_2_v1', 'tof_2_v2', 'tof_2_v3', 'tof_2_v4', 'tof_2_v5', 'tof_2_v6', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63']\n",
      "tof_3: ['tof_3_v0', 'tof_3_v1', 'tof_3_v2', 'tof_3_v3', 'tof_3_v4', 'tof_3_v5', 'tof_3_v6', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63']\n",
      "tof_4: ['tof_4_v0', 'tof_4_v1', 'tof_4_v2', 'tof_4_v3', 'tof_4_v4', 'tof_4_v5', 'tof_4_v6', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63']\n",
      "tof_5: ['tof_5_v0', 'tof_5_v1', 'tof_5_v2', 'tof_5_v3', 'tof_5_v4', 'tof_5_v5', 'tof_5_v6', 'tof_5_v7', 'tof_5_v8', 'tof_5_v9', 'tof_5_v10', 'tof_5_v11', 'tof_5_v12', 'tof_5_v13', 'tof_5_v14', 'tof_5_v15', 'tof_5_v16', 'tof_5_v17', 'tof_5_v18', 'tof_5_v19', 'tof_5_v20', 'tof_5_v21', 'tof_5_v22', 'tof_5_v23', 'tof_5_v24', 'tof_5_v25', 'tof_5_v26', 'tof_5_v27', 'tof_5_v28', 'tof_5_v29', 'tof_5_v30', 'tof_5_v31', 'tof_5_v32', 'tof_5_v33', 'tof_5_v34', 'tof_5_v35', 'tof_5_v36', 'tof_5_v37', 'tof_5_v38', 'tof_5_v39', 'tof_5_v40', 'tof_5_v41', 'tof_5_v42', 'tof_5_v43', 'tof_5_v44', 'tof_5_v45', 'tof_5_v46', 'tof_5_v47', 'tof_5_v48', 'tof_5_v49', 'tof_5_v50', 'tof_5_v51', 'tof_5_v52', 'tof_5_v53', 'tof_5_v54', 'tof_5_v55', 'tof_5_v56', 'tof_5_v57', 'tof_5_v58', 'tof_5_v59', 'tof_5_v60', 'tof_5_v61', 'tof_5_v62', 'tof_5_v63']\n",
      "Extra train columns\n",
      "['sequence_type', 'orientation', 'behavior', 'phase', 'gesture']\n",
      "sequence_type:  ['Non-Target', 'Target']\n",
      "orientation:    ['Lie on Side - Non Dominant', 'Lie on Back', 'Seated Straight', 'Seated Lean Non Dom - FACE DOWN']\n",
      "behavior:       ['Moves hand to target location', 'Performs gesture', 'Relaxes and moves hand to target location', 'Hand at target location']\n",
      "phase:          ['Transition', 'Gesture']\n",
      "gesture:        ['Forehead - scratch', 'Eyelash - pull hair', 'Text on phone', 'Write name on leg', 'Cheek - pinch skin', 'Glasses on/off', 'Pinch knee/leg skin', 'Write name in air', 'Pull air toward your face', 'Eyebrow - pull hair', 'Scratch knee/leg skin', 'Feel around in tray and pull out an object', 'Drink from bottle/cup', 'Forehead - pull hairline', 'Above ear - pull hair', 'Wave hello', 'Neck - pinch skin', 'Neck - scratch']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test columns\")\n",
    "test_cols = list(pl.read_csv(TEST_PATH).columns)\n",
    "id_cols = test_cols[:4]\n",
    "acc_cols = [col  for col in test_cols if col.startswith(\"acc\")]\n",
    "rot_cols = [col  for col in test_cols if col.startswith(\"rot\")]\n",
    "thm_cols = [col  for col in test_cols if col.startswith(\"thm\")]\n",
    "tof_cols = [[col  for col in test_cols if col.startswith(f\"tof_{i+1}\")] for i in range(5)]\n",
    "tof_cols_all = [col for cl in tof_cols for col in cl]\n",
    "target_cols = [col for col in train_ds.columns if col not in test_cols]\n",
    "features = acc_cols+rot_cols+thm_cols+[c for cl in tof_cols for c in cl]\n",
    "\n",
    "demo_features = ['adult_child', 'age', 'sex', 'handedness', 'height_cm', 'shoulder_to_wrist_cm', 'elbow_to_wrist_cm']\n",
    "\n",
    "# target\n",
    "gestures = ['Pull air toward your face', 'Feel around in tray and pull out an object', 'Neck - scratch', 'Pinch knee/leg skin', \n",
    "            'Forehead - scratch', 'Eyelash - pull hair', 'Drink from bottle/cup', 'Wave hello', 'Cheek - pinch skin', \n",
    "            'Forehead - pull hairline', 'Text on phone', 'Write name in air', 'Scratch knee/leg skin', 'Neck - pinch skin', \n",
    "            'Write name on leg', 'Above ear - pull hair', 'Eyebrow - pull hair', 'Glasses on/off']\n",
    "le = LabelEncoder()\n",
    "le.fit(gestures)\n",
    "train_ds = train_ds.with_columns(pl.Series(name=\"gesture_id\", values=le.transform(train_ds.select(\"gesture\"))))\n",
    "\n",
    "print(f\"id-cols: {id_cols}\")\n",
    "print(f\"acc: {acc_cols}\")\n",
    "print(f\"rot: {rot_cols}\")\n",
    "print(f\"thm: {thm_cols}\")\n",
    "for i in range(5):\n",
    "    print(f\"tof_{i+1}: {tof_cols[i]}\")\n",
    "    \n",
    "print(f\"Extra train columns\")\n",
    "print(f\"{target_cols}\")\n",
    "for col in target_cols:\n",
    "    print(f\"{col+':':15} {train_ds.select(col).unique().to_series().to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return (data\n",
    "            .sort(by=\"row_id\")\n",
    "            .with_columns(pl.all().fill_null(strategy=\"forward\").over(\"sequence_id\"))\n",
    "            .with_columns(pl.all().fill_null(strategy=\"backward\").over(\"sequence_id\"))\n",
    "            .with_columns(pl.col(tof_cols_all).fill_null(-1)) \n",
    "            .with_columns(pl.col(thm_cols).fill_null(strategy=\"mean\").over(\"sequence_id\"))\n",
    "            .with_columns(pl.all().fill_null(0))\n",
    "            \n",
    "            .filter(pl.col(\"sequence_id\") != 'SEQ_011975')\n",
    "            )\n",
    "train_ds_prep = preprocess(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "def split_data(data, data_demo):\n",
    "    sequences = (data\n",
    "                .group_by([\"sequence_id\", \"subject\"])\n",
    "                .agg(pl.col(\"gesture\").first())\n",
    "                )\n",
    "    sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    sgkf2 = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    train_index, test_index = next(sgkf.split(sequences, \n",
    "                                            sequences.select(\"gesture\").to_series(), \n",
    "                                            sequences.select(\"subject\").to_series() ))\n",
    "    train_index2, test_index2 = next(sgkf2.split(sequences[test_index], \n",
    "                                            sequences[test_index].select(\"gesture\").to_series(), \n",
    "                                            sequences[test_index].select(\"subject\").to_series() ))\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    for part_name, part_index in zip([\"train\", \"val\", \"test\"], [train_index, train_index2, test_index2]):\n",
    "        data_dict[part_name] = data.filter(pl.col(\"sequence_id\").is_in(sequences[part_index].select(\"sequence_id\").to_series().implode()))\n",
    "        data_dict[part_name + \"_demo\"] = data_demo.filter(pl.col(\"subject\").is_in(sequences[part_index].select(\"subject\").to_series().implode()))\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "   \n",
    "# train, test = split_dataset(train_ds_prep, with_val=False)\n",
    "# data_dict = split_dataset(train_ds_prep, with_val=True)\n",
    "\n",
    "\n",
    "\n",
    "def create_folds(data, data_demo, num_folds=5):\n",
    "    data = data.with_columns(pl.lit(-1).alias(\"FOLD\"))\n",
    "    data_demo = data_demo.with_columns(pl.lit(-1).alias(\"FOLD\"))\n",
    "    subjects = data.select(\"subject\").unique()\n",
    "    kf = KFold(n_splits=num_folds, random_state=24, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(subjects)):\n",
    "        data = data.with_columns(pl.when(pl.col(\"subject\").is_in(subjects[test_index].to_series().implode()))\n",
    "                                 .then(pl.lit(i))\n",
    "                                 .otherwise(pl.col(\"FOLD\"))\n",
    "                                 .alias(\"FOLD\"))\n",
    "        \n",
    "        data_demo = data_demo.with_columns(pl.when(pl.col(\"subject\").is_in(subjects[test_index].to_series().implode()))\n",
    "                                 .then(pl.lit(i))\n",
    "                                 .otherwise(pl.col(\"FOLD\"))\n",
    "                                 .alias(\"FOLD\"))\n",
    "    return data, data_demo\n",
    "# train_ds_2 = pl.read_csv(TRAIN_PATH)\n",
    "# create_folds(train_ds_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_dict, features=features):\n",
    "    ct = ColumnTransformer(\n",
    "            [('std', StandardScaler(), features)],\n",
    "            verbose_feature_names_out=False, \n",
    "            remainder=\"passthrough\"\n",
    "        )\n",
    "    ct.set_output(transform=\"polars\")\n",
    "\n",
    "    data_dict[\"train\"] = ct.fit_transform(data_dict[\"train\"])\n",
    "    if \"val\" in data_dict and data_dict[\"val\"] is not None:\n",
    "        data_dict[\"val\"] = ct.transform(data_dict[\"val\"])\n",
    "    \n",
    "    if \"test\" in data_dict and  data_dict[\"test\"] is not None:\n",
    "        data_dict[\"test\"] = ct.transform(data_dict[\"test\"])\n",
    "    return data_dict\n",
    "\n",
    "# data_dict = preprocess(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_kfold = GroupKFold(n_splits=5, shuffle=True, random_state=24)\n",
    "# for i, (train_index, test_index) in enumerate(group_kfold.split(train, groups=train.select(\"subject\"))):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     train_folds = train.with_row_index().filter(pl.col(\"index\").is_in(train_index))\n",
    "#     test_folds = train.with_row_index().filter(pl.col(\"index\").is_in(test_index))\n",
    "    \n",
    "#     ct = ColumnTransformer(\n",
    "#         [('std', StandardScaler(), features)],\n",
    "#         verbose_feature_names_out=False, \n",
    "#         remainder=\"passthrough\"\n",
    "#     )\n",
    "#     ct.set_output(transform=\"polars\")\n",
    "#     train_folds = ct.fit_transform(train_folds)\n",
    "#     test_folds = ct.transform(test_folds)\n",
    "    \n",
    "#     X_train = train_folds.select(features)\n",
    "#     y_train = train_folds.select(\"gesture\")\n",
    "    \n",
    "    \n",
    "#     print(f\"  Train: index={train_folds.shape[0]}, group={train_folds.select(\"subject\").unique().shape}\")\n",
    "#     print(f\"  Test:  index={test_folds.shape[0]}, group={test_folds.select(\"subject\").unique().shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train, val, test, target=\"gesture\"):\n",
    "    \n",
    "    def perpare_part(part, tail_length=75):\n",
    "        sequences = {col_name: [] for col_name in [\"acc\", \"rot\", \"thm\", \"tof\", \"target\"]}\n",
    "        for name, data in (part\n",
    "                            .sort(by=['sequence_id', 'sequence_counter'])\n",
    "                            .group_by(\"sequence_id\")\n",
    "                            .tail(tail_length)\n",
    "                            .group_by(\"sequence_id\")\n",
    "                            ):\n",
    "            \n",
    "            for col_name, cols in zip([\"acc\", \"rot\", \"thm\", \"tof\"], [acc_cols, rot_cols, thm_cols, tof_cols_all ]):\n",
    "                array = data.select(cols).to_numpy()\n",
    "                if array.shape[0] < tail_length:\n",
    "                    # TODO: Better imputation\n",
    "                    padding = np.zeros((tail_length -  array.shape[0], array.shape[1]) , dtype=float)\n",
    "                    array = np.vstack((padding, array))\n",
    "                    \n",
    "                sequences[col_name].append(array)\n",
    "            \n",
    "            sequences[\"target\"].append(data.select(\"gesture_id\").tail(1).item())\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    data = {}\n",
    "    for part_name, part in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
    "        if part is not None:\n",
    "            part_data = perpare_part(part)\n",
    "            data[part_name] = {'x_acc': np.array(part_data[\"acc\"]).astype(np.float32), \n",
    "                               'x_rot': np.array(part_data[\"rot\"]).astype(np.float32), \n",
    "                               'x_thm': np.array(part_data[\"thm\"]).astype(np.float32), \n",
    "                               'x_tof': np.array(part_data[\"tof\"]).astype(np.float32), \n",
    "                               'y': np.array(part_data[\"target\"]),\n",
    "                               }\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "# train_ds_prep =  clean_dataset(train_ds)\n",
    "# data_dict = split_dataset(train_ds_prep, with_val=True)\n",
    "# data_dict = preprocess(data_dict)\n",
    "\n",
    "# data_prep = create_dataset(data_dict[\"train\"], data_dict[\"val\"], data_dict[\"test\"])\n",
    "\n",
    "\n",
    "def perpare_part(part, part_demo, tail_length=75):\n",
    "    sequences = {col_name: [] for col_name in [\"acc\", \"rot\", \"thm\", \"tof\", \"target\", \"subject\", \"demo\"]}\n",
    "    for name, data in (part\n",
    "                        .sort(by=['sequence_id', 'sequence_counter'])\n",
    "                        .group_by(\"sequence_id\")\n",
    "                        .tail(tail_length)\n",
    "                        .group_by(\"sequence_id\")\n",
    "                        ):\n",
    "        \n",
    "        for col_name, cols in zip([\"acc\", \"rot\", \"thm\", \"tof\"], [acc_cols, rot_cols, thm_cols, tof_cols_all ]):\n",
    "            array = data.select(cols).to_numpy()\n",
    "            if array.shape[0] < tail_length:\n",
    "                # TODO: Better imputation\n",
    "                padding = np.zeros((tail_length -  array.shape[0], array.shape[1]) , dtype=float)\n",
    "                array = np.vstack((padding, array))\n",
    "                \n",
    "            sequences[col_name].append(array)\n",
    "        \n",
    "        \n",
    "        sequences[\"target\"].append(data.select(\"gesture_id\").tail(1).item())\n",
    "        subject = data.select(\"subject\").tail(1).item()\n",
    "\n",
    "        sequences[\"subject\"].append(subject)\n",
    "        sequences[\"demo\"].append(part_demo.filter(pl.col(\"subject\") == subject).select(demo_features).to_numpy())\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "def create_cv_datasets(_data, _data_demo, num_folds=5, tail_length=75):\n",
    "    data_prep = clean_dataset(_data)\n",
    "    data_prep, data_demo_prep = create_folds(data_prep, _data_demo, num_folds=num_folds)\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        data_train = data_prep.filter(pl.col(\"FOLD\") != i)\n",
    "        data_val = data_prep.filter(pl.col(\"FOLD\") == i)\n",
    "        data_dict = preprocess({\"train\": data_train, \"val\": data_val})\n",
    "        \n",
    "        data_demo_train = data_demo_prep.filter(pl.col(\"FOLD\") != i)\n",
    "        data_demo_val = data_demo_prep.filter(pl.col(\"FOLD\") == i)\n",
    "        data_demo_dict = preprocess({\"train\": data_demo_train, \"val\": data_demo_val}, features=demo_features)\n",
    "        \n",
    "        data = {}\n",
    "        for part_name, part in data_dict.items():\n",
    "            part_data = perpare_part(part, data_demo_dict[part_name], tail_length=tail_length)\n",
    "            data[part_name] = {'x_acc': np.array(part_data[\"acc\"]).astype(np.float32), \n",
    "                            'x_rot': np.array(part_data[\"rot\"]).astype(np.float32), \n",
    "                            'x_thm': np.array(part_data[\"thm\"]).astype(np.float32), \n",
    "                            'x_tof': np.array(part_data[\"tof\"]).astype(np.float32), \n",
    "                            'demo': np.array(part_data[\"demo\"]).astype(np.float32),\n",
    "                            'y': np.array(part_data[\"target\"]),\n",
    "                            }\n",
    "        \n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        # stack it together for now\n",
    "        data_torch = {\n",
    "                part: {'X': torch.as_tensor((np.dstack([(data[part][mtype]) for mtype in [\"x_acc\", \"x_rot\", \"x_thm\", \"x_tof\"]])), device=device),\n",
    "                       'X_demo': torch.as_tensor(data[part][\"demo\"], device=device),\n",
    "                       'y': torch.as_tensor(data[part][\"y\"], device=device) }\n",
    "                for part in data\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def save_fold(data_torch, fold, dir=PREP_DATA_BASE, prefix=\"\"):\n",
    "            for part, d in data_torch.items():\n",
    "                for d_part, tensor in d.items():\n",
    "                    filename = os.path.join(dir, f\"{prefix}_FOLD_{i}_{part}_{d_part}.pt\")\n",
    "                    torch.save(tensor, filename)\n",
    "        \n",
    "        save_fold(data_torch, i, prefix=\"20250628_v2\")\n",
    "        for part in data_torch.values():\n",
    "            part.clear()\n",
    "        for part in data.values():\n",
    "            part.clear()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "\n",
    "\n",
    "def load_fold(fold, dir=PREP_DATA_BASE, prefix=\"\"):\n",
    "    data_dict = {}\n",
    "    for part in (\"train\", \"val\"):\n",
    "        data_dict[part] = {}\n",
    "        for d_part in (\"X\", \"X_demo\" , \"y\"):\n",
    "            filename = os.path.join(dir, f\"{prefix}_FOLD_{i}_{part}_{d_part}.pt\")\n",
    "            data_dict[part][d_part] = torch.load(filename)\n",
    "    return data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_cv_datasets(train_ds, train_demo_ds, tail_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = load_fold(3, prefix=\"20250628_v2\")\n",
    "# print(a[\"train\"][\"X\"].shape)\n",
    "# print(a[\"train\"][\"X_demo\"].shape)\n",
    "# print(a[\"val\"][\"X\"].shape)\n",
    "# print(a[\"val\"][\"X_demo\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 100, 332])\n",
      "torch.Size([256, 1, 7])\n",
      "torch.Size([256, 18])\n",
      "tensor([[0.0527, 0.0635, 0.0545,  ..., 0.0628, 0.0535, 0.0558],\n",
      "        [0.0484, 0.0610, 0.0597,  ..., 0.0671, 0.0556, 0.0587],\n",
      "        [0.0479, 0.0574, 0.0661,  ..., 0.0617, 0.0462, 0.0472],\n",
      "        ...,\n",
      "        [0.0428, 0.0718, 0.0677,  ..., 0.0776, 0.0619, 0.0640],\n",
      "        [0.0460, 0.0606, 0.0619,  ..., 0.0688, 0.0578, 0.0601],\n",
      "        [0.0741, 0.0654, 0.0432,  ..., 0.0488, 0.0505, 0.0459]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    \n",
    "        \n",
    "    class LSTMClassifier(nn.Module):\n",
    "\n",
    "        def __init__(self, input_dim, input_demo_dim, hidden_dim, classes_dim, num_layers, dropout_rate=0):\n",
    "            super(LSTMClassifier, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.input_dim = input_dim\n",
    "            \n",
    "            self.rnn = nn.RNN(12+input_demo_dim, 12, num_layers=1, batch_first=True)\n",
    "            \n",
    "            # conv layer 1: K: 64x5, b: 64\n",
    "            self.features_maps = [64, 64, 64, 64]\n",
    "            self.kernel_sizes = [4,4,4,4]\n",
    "            # self.conv1 = nn.Conv1d(input_dim, input_dim*self.features_maps[0], self.kernel_sizes[0], stride=1, groups=input_dim)\n",
    "            self.conv1 = nn.Conv1d(1, self.features_maps[0], self.kernel_sizes[0], stride=1, groups=1)\n",
    "            \n",
    "            # conv layer 2-4:  K: 64x64x5, b: 64\n",
    "            self.conv2 = nn.Conv2d(1, self.features_maps[1], (self.features_maps[0], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "            self.conv3 = nn.Conv2d(1, self.features_maps[2], (self.features_maps[1], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "            self.conv4 = nn.Conv2d(1, self.features_maps[3], (self.features_maps[2], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "            \n",
    "            # tof conv layers:\n",
    "            self.tof_features_maps = [8,16,32]\n",
    "            self.tof_conv1 = nn.Conv3d(1, self.tof_features_maps[0], (5,3,3), stride=1)\n",
    "            self.tof_conv2 = nn.Conv3d(self.tof_features_maps[0], self.tof_features_maps[1], (5,3,3), stride=1)\n",
    "            self.tof_conv3 = nn.Conv3d(self.tof_features_maps[1], self.tof_features_maps[2], (5,3,3), stride=1)\n",
    "            self.tof_maxp = nn.MaxPool3d(kernel_size=(1,2,2))\n",
    "            \n",
    "            # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "            # with dimensionality hidden_dim.\n",
    "            self.lstm = nn.LSTM(12*self.features_maps[3] + 5*self.tof_features_maps[2], hidden_dim, num_layers, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "            # The linear layer that maps from hidden state space to tag space\n",
    "            self.hidden2class = nn.Linear(hidden_dim+input_demo_dim, classes_dim)\n",
    "\n",
    "        # x: ( BATCH_SIZE, time, features)\n",
    "        # x_demo: (BATCH_SIZE, 1, demo features)\n",
    "        def forward(self, x, x_demo):\n",
    "\n",
    "            # 1.1 only acc, rot and thm (body heat) through rnn\n",
    "            \n",
    "            # demo input to (time, batch_size, demo features) and repeat in time dimension\n",
    "            x_demo_repeated = x_demo.repeat(1, x.shape[1], 1)\n",
    "            x_in = torch.cat([x[:, :, :12], x_demo_repeated], dim=2)\n",
    "            rnn_out, _ = self.rnn(x_in)\n",
    "            \n",
    "            # swap channels and time dimension\n",
    "            # (BATCH_SIZE, channels, time)\n",
    "            rnn_out = rnn_out.transpose(1,2)\n",
    "            # channels to batch dimensions to apply same convolution per channel\n",
    "            # (BATCH_SIZE * channels, time)\n",
    "            rnn_out = rnn_out.view(-1, 1, rnn_out.shape[-1])\n",
    "            \n",
    "            \n",
    "            # 1.2 rnn output through CNN\n",
    "            x_conv = nn.functional.relu(self.conv1(rnn_out))\n",
    "            \n",
    "            for i, conv in enumerate([self.conv2, self.conv3, self.conv4]):\n",
    "                # print(f\"Before conv {i+2} \", x_conv.shape)\n",
    "                x_conv = nn.functional.relu(conv(x_conv.squeeze().unsqueeze(1)))\n",
    "                # print(f\"After conv {i+2} \", x_conv.shape)\n",
    "            \n",
    "            # After convolutions we get tensor of shape (BATCH_SIZE * channels, last-feature-map, 1, time)\n",
    "            x_conv = x_conv.squeeze(2)\n",
    "            # separate channels from batch dimenison\n",
    "            x_conv = x_conv.view(-1, 12, x_conv.shape[1], x_conv.shape[2])\n",
    "            # merge feature map dimension into channels dimension \n",
    "            x_conv = x_conv.view(x_conv.shape[0],-1, x_conv.shape[3])\n",
    "            # Transformed tensor to shape (BATCH_SIZE, channels * last-feature-map, time)\n",
    "            # -> 64*12=768 Features\n",
    "            \n",
    "            # 2.1 tof features\n",
    "            x_tof = x[:, :, 12:].view(x.shape[0],x.shape[1], 5, 8, 8)\n",
    "            \n",
    "            # 2.2. tof Features through CNN\n",
    "            # (BATCH_SIZE, time, Depth, H, W) -> (BATCH_SIZE, Depth, time, H, W)\n",
    "            x_tof = x_tof.transpose(1,2)\n",
    "            # (BATCH_SIZE, Depth, time, H, W) -> (BATCH_SIZE * Depth, time, H, W)\n",
    "            x_tof = x_tof.reshape(-1,1, x_tof.shape[2], x_tof.shape[3], x_tof.shape[4])\n",
    "            \n",
    "            # 8x8 -> 4x4x8 -> 2x2x16 -> 1x1x32 -> 32*5=160 features\n",
    "            x_tof = nn.functional.relu(self.tof_conv1(x_tof))\n",
    "            x_tof = nn.functional.relu(self.tof_conv2(x_tof))\n",
    "            x_tof = nn.functional.relu(self.tof_maxp(self.tof_conv3(x_tof)))\n",
    "            \n",
    "            # Separate Batchsize from Depth dimension\n",
    "            x_tof = x_tof.squeeze()\n",
    "            # Separate batch and channels (BATCH_SIZE, Channels, feature maps, time)\n",
    "            x_tof = x_tof.view(-1, 5, x_tof.shape[1], x_tof.shape[2])\n",
    "            # Flatten channel and feature maps dim (BATCH_SIZE, Channels*feature maps, time)\n",
    "            x_tof = x_tof.view(x_tof.shape[0],-1, x_tof.shape[3])\n",
    "            \n",
    "            # Cat acc, rot, thm and tof feature maps\n",
    "            lstm_in = torch.cat([x_conv, x_tof], dim=1)\n",
    "            lstm_out, _ = self.lstm(lstm_in.transpose(1,2))\n",
    "\n",
    "            x_cat = torch.cat([lstm_out[:,-1,:], x_demo.squeeze(1)], 1)\n",
    "            class_space = self.hidden2class(x_cat)\n",
    "            class_scores = F.softmax(class_space, dim=1)\n",
    "            return class_scores\n",
    "\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_LAYERS = 2\n",
    "    HIDDEN_SIZE = 128\n",
    "    model = LSTMClassifier(332,7, hidden_dim=HIDDEN_SIZE, classes_dim=len(le.classes_), num_layers=NUM_LAYERS , dropout_rate=0).to(device)\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    data = load_fold(0, prefix=\"20250628_v2\")\n",
    "    data[\"test\"] = data[\"val\"]\n",
    "    idx = np.random.randint(low=0, high=5000, size=BATCH_SIZE)\n",
    "    print(data[\"train\"][\"X\"][idx].shape)\n",
    "    print(data[\"train\"][\"X_demo\"][idx].shape)\n",
    "    \n",
    "    # output (L,N,D∗H): L=sequence length,  N=Batch size, H=Hiddensize\n",
    "    # hidden size (D∗num_layers,N,H): num_layers, N=Batch Size, H=hidden_size\n",
    "    def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "        return (\n",
    "            model(\n",
    "                data[part]['X'][idx],\n",
    "                data[part]['X_demo'][idx]\n",
    "            )\n",
    "        )\n",
    "    out = apply_model(\"train\", idx)\n",
    "    print(out.shape)\n",
    "    print(out)\n",
    "\n",
    " \n",
    "test_model()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': LSTMClassifier(\n",
       "   (rnn): RNN(339, 332, batch_first=True)\n",
       "   (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,))\n",
       "   (conv2): Conv2d(1, 64, kernel_size=(64, 5), stride=(1, 1))\n",
       "   (conv3): Conv2d(1, 64, kernel_size=(64, 5), stride=(1, 1))\n",
       "   (conv4): Conv2d(1, 64, kernel_size=(64, 5), stride=(1, 1))\n",
       "   (lstm): LSTM(21248, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "   (hidden2class): Linear(in_features=135, out_features=18, bias=True)\n",
       " ),\n",
       " 'eval_mode': torch.autograd.grad_mode.inference_mode,\n",
       " 'optimizer': AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.01\n",
       "     maximize: False\n",
       "     weight_decay: 0.9\n",
       " ),\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'grad_scaler': None,\n",
       " 'amp_enabled': False,\n",
       " 'amp_dtype': torch.bfloat16,\n",
       " 'target': 'gesture_id'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"compile_model\": False,\n",
    "    \n",
    "    \"n_features\": 332,\n",
    "    \"n_demo_features\": 7,\n",
    "    \"n_classes\": len(le.classes_),\n",
    "    \"target\": \"gesture_id\"\n",
    "}\n",
    "\n",
    "hyper_params = {\n",
    "    \"lstm_layers\": 2,\n",
    "    \"hidden_size\": 128,\n",
    "    'dropout': 0.5,\n",
    "    \"learning_rate\": 10e-3, \n",
    "    \"weight_decay\": 0.9,\n",
    "}\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, input_demo_dim, hidden_dim, classes_dim, num_layers, dropout_rate=0):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim+input_demo_dim, input_dim, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # conv layer 1: K: 64x5, b: 64\n",
    "        self.features_maps = [64, 64, 64, 64]\n",
    "        self.kernel_sizes = [5,5,5,5]\n",
    "        # self.conv1 = nn.Conv1d(input_dim, input_dim*self.features_maps[0], self.kernel_sizes[0], stride=1, groups=input_dim)\n",
    "        self.conv1 = nn.Conv1d(1, self.features_maps[0], self.kernel_sizes[0], stride=1, groups=1)\n",
    "        \n",
    "        # conv layer 2-4:  K: 64x64x5, b: 64\n",
    "        self.conv2 = nn.Conv2d(1, self.features_maps[1], (self.features_maps[0], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "        self.conv3 = nn.Conv2d(1, self.features_maps[2], (self.features_maps[1], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "        self.conv4 = nn.Conv2d(1, self.features_maps[3], (self.features_maps[2], self.kernel_sizes[1]), stride=1, groups=1)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim*self.features_maps[3], hidden_dim, num_layers, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2class = nn.Linear(hidden_dim+input_demo_dim, classes_dim)\n",
    "\n",
    "    # x: ( BATCH_SIZE, time, features)\n",
    "    # x_demo: (BATCH_SIZE, 1, demo features)\n",
    "    def forward(self, x, x_demo):\n",
    "        # demo input to (time, batch_size, demo features) and repeat in time dimension\n",
    "        x_demo_repeated = x_demo.repeat(1, x.shape[1], 1)\n",
    "        x_in = torch.cat([x, x_demo_repeated], dim=2)\n",
    "        rnn_out, _ = self.rnn(x_in)\n",
    "        \n",
    "        # swap channels and time dimension\n",
    "        # (BATCH_SIZE, channels, time)\n",
    "        rnn_out = rnn_out.transpose(1,2)\n",
    "        # channels to batch dimensions to apply same convolution per channel\n",
    "        # (BATCH_SIZE * channels, time)\n",
    "        rnn_out = rnn_out.view(-1, 1, rnn_out.shape[-1])\n",
    "        \n",
    "        x_conv = nn.functional.relu(self.conv1(rnn_out))\n",
    "        \n",
    "        for i, conv in enumerate([self.conv2, self.conv3, self.conv4]):\n",
    "            # print(f\"Before conv {i+2} \", x_conv.shape)\n",
    "            x_conv = nn.functional.relu(conv(x_conv.squeeze().unsqueeze(1)))\n",
    "            # print(f\"After conv {i+2} \", x_conv.shape)\n",
    "        \n",
    "        # After convolutions we get tensor of shape (BATCH_SIZE * channels, last-feature-map, 1, time)\n",
    "        x_conv = x_conv.squeeze()\n",
    "        # separate channels from batch dimenison\n",
    "        x_conv = x_conv.view(-1, self.input_dim, x_conv.shape[1], x_conv.shape[2])\n",
    "        # merge feature map dimension into channels dimension \n",
    "        x_conv = x_conv.view(x_conv.shape[0],-1, x_conv.shape[3])\n",
    "        # Transformed tensor to shape (BATCH_SIZE, channels * last-feature-map, time)\n",
    "        \n",
    "        # x_conv = x_conv.reshape((x_conv.shape[0], self.input_dim, -1, x_conv.shape[2]))\n",
    "        # print(x_conv.shape)\n",
    "        # print(x_conv[0, 0,0,:])\n",
    "        lstm_out, _ = self.lstm(x_conv.transpose(1,2))\n",
    "        \n",
    "        x_cat = torch.cat([lstm_out[:,-1,:], x_demo.squeeze(1)], 1)\n",
    "        class_space = self.hidden2class(x_cat)\n",
    "        class_scores = F.softmax(class_space, dim=1)\n",
    "        return class_scores\n",
    "\n",
    "\n",
    "def prepare_model(config):\n",
    "    # Device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Automatic mixed precision (AMP)\n",
    "    # torch.float16 is implemented for completeness,\n",
    "    # but it was not tested in the project,\n",
    "    # so torch.bfloat16 is used by default.\n",
    "    amp_dtype = (\n",
    "        torch.bfloat16\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        else torch.float16\n",
    "        if torch.cuda.is_available()\n",
    "        else None\n",
    "    )\n",
    "    # Changing False to True will result in faster training on compatible hardware.\n",
    "    amp_enabled = False and amp_dtype is not None\n",
    "    grad_scaler = torch.amp.GradScaler(\"cuda\") if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "    # torch.compile\n",
    "    compile_model = config[\"compile_model\"]\n",
    "\n",
    "    # fmt: off\n",
    "    print(\n",
    "        f'Device:        {device.type.upper()}'\n",
    "        f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "        f'\\ntorch.compile: {compile_model}'\n",
    "    )\n",
    "    \n",
    "    # Choose one of the two configurations below.\n",
    "    # TODO\n",
    "    model = LSTMClassifier(config[\"n_features\"], config[\"n_demo_features\"], config[\"hidden_size\"], config[\"n_classes\"], config[\"lstm_layers\"], config[\"dropout\"]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    if compile_model:\n",
    "        # NOTE\n",
    "        # `torch.compile` is intentionally called without the `mode` argument\n",
    "        # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "        model = torch.compile(model)\n",
    "        evaluation_mode = torch.no_grad\n",
    "    else:\n",
    "        evaluation_mode = torch.inference_mode\n",
    "        \n",
    "    model_dict = {\"model\": model,\n",
    "                  \"eval_mode\": evaluation_mode,\n",
    "                  \"optimizer\": optimizer,\n",
    "                  \"device\": device,\n",
    "                  \"grad_scaler\": grad_scaler,\n",
    "                  \"amp_enabled\": amp_enabled,\n",
    "                  \"amp_dtype\": amp_dtype,\n",
    "                  \"target\": config[\"target\"]\n",
    "                  }\n",
    "        \n",
    "    return model_dict\n",
    "\n",
    "prepare_model(CONFIG | hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/29 21:53:22 INFO mlflow.tracking.fluent: Experiment with name 'CMI LSTM Experiment 2025-06-29 21:53:22.915970' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model_dict, data, config, verbose=False):\n",
    "    model = model_dict[\"model\"]\n",
    "    optimizer = model_dict[\"optimizer\"]\n",
    "    evaluation_mode = model_dict[\"eval_mode\"]\n",
    "    device = model_dict[\"device\"]\n",
    "    grad_scaler = model_dict[\"grad_scaler\"]\n",
    "    amp_enabled = model_dict[\"amp_enabled\"]\n",
    "    amp_dtype = model_dict[\"amp_dtype\"]\n",
    "    target = model_dict[\"target\"]\n",
    "    \n",
    "    \n",
    "    @torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "    def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "        return (\n",
    "            model(\n",
    "                data[part]['X'][idx],\n",
    "                data[part]['X_demo'][idx]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    task_type = \"classification\"\n",
    "    base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
    "\n",
    "\n",
    "    def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "        return base_loss_fn(y_pred, y_true)\n",
    "\n",
    "    def score_fn(y_true, y_pred):\n",
    "        sol = pd.DataFrame({\"gesture\": le.inverse_transform(y_true)}).reset_index(names=[\"id\"])\n",
    "        sub = pd.DataFrame({\"gesture\": le.inverse_transform(y_pred)}).reset_index(names=[\"id\"])\n",
    "        return score(sol, sub, row_id_column_name='id')\n",
    "\n",
    "    @evaluation_mode()\n",
    "    def evaluate(part: str) -> tuple[float, float]:\n",
    "        model.eval()\n",
    "\n",
    "        # When using torch.compile, you may need to reduce the evaluation batch size.\n",
    "        eval_batch_size = 8096\n",
    "        y_pred = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    apply_model(part, idx)\n",
    "                    for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "                        eval_batch_size\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_pred, data[part][\"y\"]).detach().cpu().numpy()\n",
    "\n",
    "        if task_type != 'regression':\n",
    "            # For classification, the mean must be computed in the probabily space.\n",
    "            y_pred = F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "        y_true = data[part]['y'].cpu().numpy()\n",
    "        \n",
    "        sc = (\n",
    "            score_fn(y_true, y_pred.argmax(1))\n",
    "        )\n",
    "        return float(sc), float(loss)  # The higher -- the better.\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Test score before training: {evaluate(\"test\")[0]:.4f}')\n",
    "    \n",
    "    # For demonstration purposes (fast training and bad performance),\n",
    "    # one can set smaller values:\n",
    "    # n_epochs = 20\n",
    "    # patience = 2\n",
    "    n_epochs = 1_000_000_000\n",
    "    if \"n_epochs\" in config:\n",
    "        n_epochs =  config[\"n_epochs\"]\n",
    "    \n",
    "    # Early stopping: the training stops when\n",
    "    # there are more than `patience` consequtive bad updates.\n",
    "    patience = 10\n",
    "    if \"patience\" in config:\n",
    "        patience =  config[\"patience\"]\n",
    "    \n",
    "\n",
    "    batch_size = 256\n",
    "    epoch_size = math.ceil(len(data[\"train\"][\"X\"]) / batch_size)\n",
    "    best = {\n",
    "        'val': -math.inf,\n",
    "        'test': -math.inf,\n",
    "        'epoch': -1,\n",
    "    }\n",
    "    \n",
    "    remaining_patience = patience\n",
    "\n",
    "    if verbose:\n",
    "        print('-' * 88 + '\\n')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            pred_train = torch.zeros((len(data[\"train\"][\"X\"]), config[\"n_classes\"]), device=device)\n",
    "            for batch_idx in tqdm(\n",
    "                torch.randperm(len(data['train']['y']), device=device).split(batch_size),\n",
    "                desc=f'Epoch {epoch}',\n",
    "                total=epoch_size,\n",
    "                disable=not verbose\n",
    "            ):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                pred = apply_model('train', batch_idx)\n",
    "                loss = loss_fn(pred, data[\"train\"][\"y\"][batch_idx])\n",
    "                pred_train[batch_idx] = pred.detach()\n",
    "                if grad_scaler is None:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    grad_scaler.scale(loss).backward()  # type: ignore\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "                    \n",
    "            train_loss = loss_fn(pred_train, data[\"train\"][\"y\"]).cpu().numpy()\n",
    "            train_score = float(score_fn(data[\"train\"][\"y\"].cpu().numpy(), \n",
    "                                         F.softmax(pred_train, dim=1).cpu().numpy().argmax(1)) )\n",
    "            \n",
    "            val_score, val_loss = evaluate('val')\n",
    "            test_score, test_loss = evaluate('test')\n",
    "            if verbose:\n",
    "                print(f'(val) {val_score:.4f} (test) {test_score:.4f}')\n",
    "\n",
    "            mlflow.log_metrics({\"train_loss\": float(train_loss), \"val_loss\": val_loss, \"test_loss\": test_loss,\n",
    "                                \"train_f1-score\": train_score, \"val_f1-score\": val_score, \"test_f1-score\": test_score,  \n",
    "                                }, step=epoch)\n",
    "            \n",
    "            \n",
    "            # if patience is set to 0, don't do early stopping\n",
    "            if (val_score > best['val']) or (patience == 0):\n",
    "                if verbose:\n",
    "                    print('🌸 New best epoch! 🌸')\n",
    "                best = {'train': train_score, 'val': val_score, 'test': test_score, 'epoch': epoch}\n",
    "                \n",
    "                # mlflow.pytorch.log_model(pytorch_model=model_dict[\"model\"], \n",
    "                #                          artifact_path=\"\", \n",
    "                #                          registered_model_name=f\"model_{epoch}\",\n",
    "                #                          input_example=data[\"train\"][\"X\"][0,:,:].cpu().numpy())\n",
    "                \n",
    "                \n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "\n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "            \n",
    "            if verbose:\n",
    "                print()\n",
    "        \n",
    "        mlflow.log_metrics({ \"best_train_score\": best[\"train\"], \"best_val_score\": best[\"val\"], \"best_test_score\": best[\"test\"], \n",
    "                              \"best_epoch\": best[\"epoch\"] })\n",
    "        if verbose:\n",
    "            print('\\n\\nResult:')\n",
    "            print(best)\n",
    "    return best\n",
    "\n",
    "\n",
    "mlflow.set_experiment(f\"CMI LSTM Experiment {pd.Timestamp.now()}\")\n",
    "metrics = pl.DataFrame()\n",
    "for i in range(5):\n",
    "    print(f\"FOLD {i}\")\n",
    "    data = load_fold(i, prefix=\"20250628_v2\")\n",
    "    data[\"test\"] = data[\"val\"]\n",
    "\n",
    "    model_dict = prepare_model(config=CONFIG | hyper_params)\n",
    "\n",
    "    best = train(model_dict, data, CONFIG | hyper_params, verbose=True)\n",
    "    print(best)\n",
    "    metrics = pl.concat([metrics, pl.DataFrame(best)])\n",
    "    \n",
    "    data.clear()\n",
    "    model_dict.clear() \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "print(metrics)\n",
    "print(metrics.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick grid search \n",
    "hyper_params = {\n",
    "    \"lstm_layers\": 1,\n",
    "    \"hidden_size\": 32,\n",
    "    'dropout': 0.1,\n",
    "    \"learning_rate\": 2e-3, \n",
    "    \"weight_decay\": 3e-4,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"lstm_layers\": [1,2,4,8,16],\n",
    "    \"hidden_size\": [32, 64, 128, 256],\n",
    "    \"dropout\": [0, 0.1, 0.25, 0.5],\n",
    "    \"learning_rate\": [1e-1, 1e-2, 1e-3]\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(f\"CMI LSTM Gridsearch {pd.Timestamp.now()}\")\n",
    "gs_results = pd.DataFrame()\n",
    "\n",
    "for params in tqdm(ParameterGrid(param_grid)):\n",
    "    hp_run = hyper_params.copy()\n",
    "    hp_run.update(params)\n",
    "    model_dict, data = prepare_model(data_prep, config=CONFIG | hp_run)\n",
    "    \n",
    "    final_scores = train(model_dict, data, CONFIG | hp_run, verbose=False)\n",
    "    hp_run.update(final_scores)\n",
    "    gs_results = pd.concat([gs_results, pd.DataFrame(hp_run, index=[0])], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lstm_layers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hidden_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dropout",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weight_decay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "train",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "epoch",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "03c8ed5c-7c99-4db2-8643-0990fb4c5178",
       "rows": [
        [
         "0",
         "1",
         "256",
         "0.1",
         "0.001",
         "0.0003",
         "0.99790293238827",
         "0.7418729775516113",
         "0.6945253669909666",
         "41"
        ],
        [
         "0",
         "2",
         "256",
         "0.0",
         "0.001",
         "0.0003",
         "0.9277471174492549",
         "0.7411609532611284",
         "0.698364585545563",
         "19"
        ],
        [
         "0",
         "4",
         "128",
         "0.25",
         "0.001",
         "0.0003",
         "0.9409899592992825",
         "0.7375230383879743",
         "0.6929088519284984",
         "41"
        ],
        [
         "0",
         "2",
         "256",
         "0.5",
         "0.001",
         "0.0003",
         "0.9539207900726789",
         "0.7372162378060214",
         "0.7112937668580548",
         "23"
        ],
        [
         "0",
         "4",
         "256",
         "0.5",
         "0.001",
         "0.0003",
         "0.9440087021587087",
         "0.7366358806588604",
         "0.6809671280437634",
         "35"
        ],
        [
         "0",
         "4",
         "256",
         "0.25",
         "0.001",
         "0.0003",
         "0.9633990175053181",
         "0.7358485400872328",
         "0.7097993955315498",
         "37"
        ],
        [
         "0",
         "2",
         "128",
         "0.5",
         "0.001",
         "0.0003",
         "0.9745231652616577",
         "0.7355017688543888",
         "0.707982570862155",
         "44"
        ],
        [
         "0",
         "4",
         "128",
         "0.5",
         "0.001",
         "0.0003",
         "0.9203556579126775",
         "0.7336872748331077",
         "0.6980872347859255",
         "35"
        ],
        [
         "0",
         "4",
         "256",
         "0.0",
         "0.001",
         "0.0003",
         "0.9302099673121362",
         "0.733421922318148",
         "0.688682938467336",
         "30"
        ],
        [
         "0",
         "2",
         "256",
         "0.25",
         "0.001",
         "0.0003",
         "0.9511214401273091",
         "0.7332584115470513",
         "0.7031123583631836",
         "23"
        ],
        [
         "0",
         "2",
         "128",
         "0.1",
         "0.001",
         "0.0003",
         "0.934947018379546",
         "0.7307462065558599",
         "0.7021844268851527",
         "29"
        ],
        [
         "0",
         "2",
         "64",
         "0.5",
         "0.001",
         "0.0003",
         "0.9143818860228398",
         "0.7306683893337511",
         "0.7001974993032614",
         "36"
        ],
        [
         "0",
         "8",
         "256",
         "0.5",
         "0.001",
         "0.0003",
         "0.9556192676932145",
         "0.7303690224088124",
         "0.6895704376133499",
         "85"
        ],
        [
         "0",
         "8",
         "128",
         "0.5",
         "0.001",
         "0.0003",
         "0.9008962931876185",
         "0.7292723958938485",
         "0.6957923950581393",
         "81"
        ],
        [
         "0",
         "1",
         "256",
         "0.25",
         "0.001",
         "0.0003",
         "0.9811948768806127",
         "0.7290676342946385",
         "0.7014610364847881",
         "28"
        ],
        [
         "0",
         "2",
         "128",
         "0.25",
         "0.001",
         "0.0003",
         "0.9542283883178135",
         "0.7278348979871121",
         "0.6947275253030667",
         "29"
        ],
        [
         "0",
         "4",
         "256",
         "0.1",
         "0.001",
         "0.0003",
         "0.9409695819407312",
         "0.727301081133949",
         "0.7061687636961924",
         "26"
        ],
        [
         "0",
         "4",
         "128",
         "0.0",
         "0.001",
         "0.0003",
         "0.9424317604724335",
         "0.7268738894873823",
         "0.6932241486605601",
         "35"
        ],
        [
         "0",
         "2",
         "256",
         "0.1",
         "0.001",
         "0.0003",
         "0.9257931475885937",
         "0.7265876854718645",
         "0.6986377237354847",
         "19"
        ],
        [
         "0",
         "4",
         "64",
         "0.0",
         "0.001",
         "0.0003",
         "0.9476139162692678",
         "0.7252387447058557",
         "0.6917708237270358",
         "53"
        ],
        [
         "0",
         "2",
         "32",
         "0.5",
         "0.01",
         "0.0003",
         "0.8971685294074848",
         "0.7252122229528205",
         "0.6823154810122584",
         "56"
        ],
        [
         "0",
         "1",
         "256",
         "0.5",
         "0.001",
         "0.0003",
         "0.9749406095304042",
         "0.7247081866703213",
         "0.6968959037190048",
         "24"
        ],
        [
         "0",
         "1",
         "128",
         "0.5",
         "0.001",
         "0.0003",
         "0.921373254060579",
         "0.7238605026645621",
         "0.698712501268215",
         "19"
        ],
        [
         "0",
         "1",
         "256",
         "0.0",
         "0.001",
         "0.0003",
         "0.8931608184739244",
         "0.7237249633334248",
         "0.7042472688654797",
         "11"
        ],
        [
         "0",
         "4",
         "64",
         "0.1",
         "0.001",
         "0.0003",
         "0.8615241382379191",
         "0.7236954772643545",
         "0.6916761051049558",
         "29"
        ],
        [
         "0",
         "4",
         "64",
         "0.25",
         "0.001",
         "0.0003",
         "0.9187395946625888",
         "0.7233458388003855",
         "0.700672064622613",
         "44"
        ],
        [
         "0",
         "1",
         "128",
         "0.0",
         "0.001",
         "0.0003",
         "0.9844424617970212",
         "0.7230522180704582",
         "0.6950725607265189",
         "34"
        ],
        [
         "0",
         "1",
         "128",
         "0.1",
         "0.001",
         "0.0003",
         "0.9407220200901136",
         "0.722923308710485",
         "0.6967357670079719",
         "21"
        ],
        [
         "0",
         "4",
         "128",
         "0.25",
         "0.01",
         "0.0003",
         "0.8425571447636288",
         "0.7216458371932114",
         "0.6846201843307838",
         "50"
        ],
        [
         "0",
         "2",
         "64",
         "0.5",
         "0.01",
         "0.0003",
         "0.8673523880297402",
         "0.7215560710195124",
         "0.7047650609020573",
         "31"
        ],
        [
         "0",
         "2",
         "32",
         "0.25",
         "0.001",
         "0.0003",
         "0.8834094260355603",
         "0.7211170290202129",
         "0.6765424511974932",
         "54"
        ],
        [
         "0",
         "2",
         "128",
         "0.0",
         "0.001",
         "0.0003",
         "0.9132713754386887",
         "0.7196861754884312",
         "0.7009551994026132",
         "23"
        ],
        [
         "0",
         "4",
         "128",
         "0.1",
         "0.01",
         "0.0003",
         "0.8733741308104307",
         "0.7190974416605678",
         "0.6740668245505004",
         "44"
        ],
        [
         "0",
         "2",
         "64",
         "0.25",
         "0.001",
         "0.0003",
         "0.9067731269594725",
         "0.718573740811265",
         "0.6867407240671562",
         "37"
        ],
        [
         "0",
         "4",
         "64",
         "0.5",
         "0.001",
         "0.0003",
         "0.8806205686751518",
         "0.718262748307673",
         "0.6921002509034674",
         "36"
        ],
        [
         "0",
         "4",
         "128",
         "0.1",
         "0.001",
         "0.0003",
         "0.8470471573761877",
         "0.7175990819170324",
         "0.6967349801316621",
         "16"
        ],
        [
         "0",
         "2",
         "64",
         "0.0",
         "0.001",
         "0.0003",
         "0.8996006521210852",
         "0.7170087528449678",
         "0.6771854514686438",
         "28"
        ],
        [
         "0",
         "4",
         "32",
         "0.0",
         "0.01",
         "0.0003",
         "0.8542238433659998",
         "0.7164421022263792",
         "0.6825046892914213",
         "49"
        ],
        [
         "0",
         "2",
         "64",
         "0.1",
         "0.001",
         "0.0003",
         "0.9065565189815883",
         "0.7135508933335737",
         "0.6916497769779766",
         "30"
        ],
        [
         "0",
         "4",
         "256",
         "0.25",
         "0.01",
         "0.0003",
         "0.8553314865207166",
         "0.7127544851929082",
         "0.6831372595517687",
         "45"
        ],
        [
         "0",
         "1",
         "128",
         "0.25",
         "0.001",
         "0.0003",
         "0.9512162612988373",
         "0.7125742914711186",
         "0.6947448463013226",
         "23"
        ],
        [
         "0",
         "2",
         "64",
         "0.25",
         "0.01",
         "0.0003",
         "0.8694886827992371",
         "0.7125381880010889",
         "0.6901543023803947",
         "34"
        ],
        [
         "0",
         "2",
         "128",
         "0.25",
         "0.01",
         "0.0003",
         "0.8946645149347205",
         "0.7111167173880191",
         "0.6857026262708551",
         "36"
        ],
        [
         "0",
         "4",
         "32",
         "0.5",
         "0.01",
         "0.0003",
         "0.8416781970544338",
         "0.7106218851360575",
         "0.6701963892015519",
         "69"
        ],
        [
         "0",
         "4",
         "64",
         "0.0",
         "0.01",
         "0.0003",
         "0.8601782265180185",
         "0.710512253878941",
         "0.6780949023863365",
         "43"
        ],
        [
         "0",
         "2",
         "32",
         "0.25",
         "0.01",
         "0.0003",
         "0.8852615367065438",
         "0.7093052012842248",
         "0.6878984565648394",
         "41"
        ],
        [
         "0",
         "2",
         "32",
         "0.5",
         "0.001",
         "0.0003",
         "0.863573749258013",
         "0.7092884767044728",
         "0.6767218364444603",
         "54"
        ],
        [
         "0",
         "1",
         "64",
         "0.1",
         "0.001",
         "0.0003",
         "0.9788825631875936",
         "0.7087847667114313",
         "0.6873003944176379",
         "58"
        ],
        [
         "0",
         "4",
         "64",
         "0.5",
         "0.01",
         "0.0003",
         "0.7847171543645272",
         "0.7077180165844776",
         "0.6611442296364058",
         "51"
        ],
        [
         "0",
         "4",
         "32",
         "0.1",
         "0.01",
         "0.0003",
         "0.8016851466544667",
         "0.707221434370465",
         "0.6960661027735094",
         "34"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 240
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lstm_layers</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.997903</td>\n",
       "      <td>0.741873</td>\n",
       "      <td>0.694525</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.927747</td>\n",
       "      <td>0.741161</td>\n",
       "      <td>0.698365</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.940990</td>\n",
       "      <td>0.737523</td>\n",
       "      <td>0.692909</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.953921</td>\n",
       "      <td>0.737216</td>\n",
       "      <td>0.711294</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.736636</td>\n",
       "      <td>0.680967</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.381525</td>\n",
       "      <td>0.394450</td>\n",
       "      <td>0.393623</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.412665</td>\n",
       "      <td>0.394450</td>\n",
       "      <td>0.393623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.409881</td>\n",
       "      <td>0.394450</td>\n",
       "      <td>0.393623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.396385</td>\n",
       "      <td>0.394378</td>\n",
       "      <td>0.393623</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.349202</td>\n",
       "      <td>0.394378</td>\n",
       "      <td>0.393623</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lstm_layers  hidden_size  dropout  learning_rate  weight_decay     train  \\\n",
       "0             1          256     0.10          0.001        0.0003  0.997903   \n",
       "0             2          256     0.00          0.001        0.0003  0.927747   \n",
       "0             4          128     0.25          0.001        0.0003  0.940990   \n",
       "0             2          256     0.50          0.001        0.0003  0.953921   \n",
       "0             4          256     0.50          0.001        0.0003  0.944009   \n",
       "..          ...          ...      ...            ...           ...       ...   \n",
       "0             8          256     0.50          0.100        0.0003  0.381525   \n",
       "0            16          256     0.50          0.001        0.0003  0.412665   \n",
       "0             8          256     0.50          0.010        0.0003  0.409881   \n",
       "0             8          128     0.00          0.100        0.0003  0.396385   \n",
       "0            16          256     0.50          0.100        0.0003  0.349202   \n",
       "\n",
       "         val      test  epoch  \n",
       "0   0.741873  0.694525     41  \n",
       "0   0.741161  0.698365     19  \n",
       "0   0.737523  0.692909     41  \n",
       "0   0.737216  0.711294     23  \n",
       "0   0.736636  0.680967     35  \n",
       "..       ...       ...    ...  \n",
       "0   0.394450  0.393623      5  \n",
       "0   0.394450  0.393623      0  \n",
       "0   0.394450  0.393623      4  \n",
       "0   0.394378  0.393623      2  \n",
       "0   0.394378  0.393623      2  \n",
       "\n",
       "[240 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_results.sort_values(by=[\"val\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    \"lstm_layers\": 2,\n",
    "    \"hidden_size\": 256,\n",
    "    'dropout': 0.5,\n",
    "    \"learning_rate\": 0.001, \n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"n_epochs\": 23,\n",
    "    \"patience\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi-behavior-sensor-data (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
